{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset and Install Libraries\n"
      ],
      "metadata": {
        "id": "g9J55CzMyMTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgsef7xW9ebP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Magneto/modified-medical-dialogue-soap-summary\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/medical_summarization\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "7jiNuJMm9kk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify the relevant entities from data and create filtered CSV for each"
      ],
      "metadata": {
        "id": "jJy3iG4zyZMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"blaze999/Medical-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"blaze999/Medical-NER\")\n",
        "\n",
        "# Create an NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "texts=ds[\"train\"][\"output\"]\n",
        "\n",
        "\n",
        "S_entities=[]\n",
        "O_entities=[]\n",
        "A_entities=[]\n",
        "P_entities=[]\n",
        "sum=0\n",
        "count=0\n",
        "for text in texts:\n",
        "  text=text.split(\"\\n\")\n",
        "  sum=0\n",
        "  count+=1\n",
        "  for i in text:\n",
        "      sum+=1\n",
        "      entities_sample=ner_pipeline(i)\n",
        "      if sum==1:\n",
        "        for a in entities_sample:\n",
        "            S_entities.append([a[\"entity\"],a[\"word\"]])\n",
        "      if sum==2:\n",
        "        for a in entities_sample:\n",
        "            O_entities.append([a[\"entity\"],a[\"word\"]])\n",
        "      if sum==3:\n",
        "        for a in entities_sample:\n",
        "            A_entities.append([a[\"entity\"],a[\"word\"]])\n",
        "\n",
        "      if sum==4:\n",
        "        for a in entities_sample:\n",
        "            P_entities.append([a[\"entity\"],a[\"word\"]])\n",
        "\n",
        "      print(\"Sample no.\",count)\n",
        "      print(len(S_entities))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4FZaZwQQ9iKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the existing CSV files\n",
        "S_path = '/content/drive/MyDrive/MT_speech/SOAP_Files/S_entities.csv'\n",
        "O_path = '/content/drive/MyDrive/MT_speech/SOAP_Files/O_entities.csv'\n",
        "A_path = '/content/drive/MyDrive/MT_speech/SOAP_Files/A_entities.csv'\n",
        "P_path = '/content/drive/MyDrive/MT_speech/SOAP_Files/P_entities.csv'\n",
        "\n",
        "# New data to be added\n",
        "def update_csv(file_path, new_entries):\n",
        "    try:\n",
        "        # Load the existing data\n",
        "        existing_data = pd.read_csv(file_path)\n",
        "\n",
        "        # Append new data\n",
        "        updated_data = pd.concat([existing_data, pd.DataFrame(new_entries, columns=existing_data.columns)], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "        # Save the updated data back to the file\n",
        "        updated_data.to_csv(file_path, index=False)\n",
        "        print(f\"Updated {file_path} successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        # If the file doesn't exist, create a new one\n",
        "        pd.DataFrame(new_entries, columns=['Entity']).to_csv(file_path, index=False)\n",
        "        print(f\"Created {file_path} with new data.\")\n",
        "\n",
        "# Update each file with corresponding list data\n",
        "update_csv(S_path, [{'Entity': entry} for entry in S_entities])\n",
        "update_csv(O_path, [{'Entity': entry} for entry in O_entities])\n",
        "update_csv(A_path, [{'Entity': entry} for entry in A_entities])\n",
        "update_csv(P_path, [{'Entity': entry} for entry in P_entities])"
      ],
      "metadata": {
        "id": "1IWvHCICKqJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S={}\n",
        "O={}\n",
        "A={}\n",
        "P={}\n",
        "\n",
        "import pandas as pd\n",
        "S_df = pd.read_csv('/content/drive/MyDrive/MT_speech/SOAP_Files/S_entities.csv')\n",
        "o_df = pd.read_csv('/content/drive/MyDrive/MT_speech/SOAP_Files/O_entities.csv')\n",
        "a_df = pd.read_csv('/content/drive/MyDrive/MT_speech/SOAP_Files/A_entities.csv')\n",
        "p_df = pd.read_csv('/content/drive/MyDrive/MT_speech/SOAP_Files/P_entities.csv')\n",
        "\n",
        "\n",
        "for index, row in S_df.iterrows():\n",
        "    entity = row['Entity']\n",
        "    entity=entity.replace(\"B-\",\"\")\n",
        "    entity=entity.replace(\"I-\",\"\")\n",
        "    if entity not in S:\n",
        "        S[entity]=1\n",
        "    else:\n",
        "        S[entity]+=1\n",
        "\n",
        "for index, row in o_df.iterrows():\n",
        "    entity = row['Entity']\n",
        "    entity=entity.replace(\"B-\",\"\")\n",
        "    entity=entity.replace(\"I-\",\"\")\n",
        "    if entity not in O:\n",
        "        O[entity]=1\n",
        "    else:\n",
        "        O[entity]+=1\n",
        "\n",
        "for index, row in a_df.iterrows():\n",
        "    entity = row['Entity']\n",
        "    entity=entity.replace(\"B-\",\"\")\n",
        "    entity=entity.replace(\"I-\",\"\")\n",
        "    if entity not in A:\n",
        "        A[entity]=1\n",
        "    else:\n",
        "        A[entity]+=1\n",
        "\n",
        "for index, row in p_df.iterrows():\n",
        "    entity = row['Entity']\n",
        "    entity=entity.replace(\"B-\",\"\")\n",
        "    entity=entity.replace(\"I-\",\"\")\n",
        "    if entity not in P:\n",
        "        P[entity]=1\n",
        "    else:\n",
        "        P[entity]+=1\n",
        "\n",
        "S,O,A,P"
      ],
      "metadata": {
        "id": "uJBQYp3I9oa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dictionary to check frequency of words and entities for each category in train"
      ],
      "metadata": {
        "id": "bPc25kxHza6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_W = {}\n",
        "O_W = {}\n",
        "A_W = {}\n",
        "P_W = {}\n",
        "\n",
        "for index, row in S_df.iterrows():\n",
        "    word = row['Word']\n",
        "    word=str(word)\n",
        "    if word==\"nan\":\n",
        "      continue\n",
        "    word = word.replace(\"B-\", \"\")\n",
        "    word = word.replace(\"I-\", \"\")\n",
        "    word=word.lower()\n",
        "    word=word.replace(\"-\",\"\")\n",
        "    word=word.replace(\"笆―",\"\")\n",
        "    if len(word)<=1:\n",
        "      continue\n",
        "    if word not in S_W:\n",
        "        S_W[word] = 1\n",
        "    else:\n",
        "        S_W[word] += 1\n",
        "\n",
        "for index, row in o_df.iterrows():\n",
        "    word = row['Word']\n",
        "    word=str(word)\n",
        "    if word==\"nan\":\n",
        "      continue\n",
        "    word = word.replace(\"B-\", \"\")\n",
        "    word = word.replace(\"I-\", \"\")\n",
        "    word=word.lower()\n",
        "    word=word.replace(\"-\",\"\")\n",
        "    word=word.replace(\"笆―",\"\")\n",
        "    if len(word)<=1:\n",
        "      continue\n",
        "    if word not in O_W:\n",
        "        O_W[word] = 1\n",
        "    else:\n",
        "        O_W[word] += 1\n",
        "\n",
        "for index, row in a_df.iterrows():\n",
        "    word = row['Word']\n",
        "    word=str(word)\n",
        "    if word==\"nan\":\n",
        "      continue\n",
        "    word = word.replace(\"B-\", \"\")\n",
        "    word = word.replace(\"I-\", \"\")\n",
        "    word=word.lower()\n",
        "    word=word.replace(\"-\",\"\")\n",
        "    word=word.replace(\"笆―",\"\")\n",
        "    if len(word)<=1:\n",
        "      continue\n",
        "    if word not in A_W:\n",
        "        A_W[word] = 1\n",
        "    else:\n",
        "        A_W[word] += 1\n",
        "\n",
        "for index, row in p_df.iterrows():\n",
        "    word = row['Word']\n",
        "    word=str(word)\n",
        "    if word==\"nan\":\n",
        "      continue\n",
        "    word = word.replace(\"B-\", \"\")\n",
        "    word = word.replace(\"I-\", \"\")\n",
        "    word=word.lower()\n",
        "    word=word.replace(\"-\",\"\")\n",
        "    word=word.replace(\"笆―",\"\")\n",
        "    if len(word)<=1:\n",
        "      continue\n",
        "    if word not in P_W:\n",
        "        P_W[word] = 1\n",
        "    else:\n",
        "        P_W[word] += 1\n",
        "\n",
        "S_W, O_W, A_W, P_W"
      ],
      "metadata": {
        "id": "lEuZ5wKnM67T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S_top = sorted(S_W.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "O_top = sorted(O_W.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "A_top = sorted(A_W.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "P_top = sorted(P_W.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "# Display the sorted top 10 keys for each dictionary\n",
        "S_top, O_top, A_top, P_top"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB2oCqa0NBV_",
        "outputId": "6fa5469f-4680-4e08-ab95-0253457a7888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('pain', 219),\n",
              "  ('of', 190),\n",
              "  ('year', 186),\n",
              "  ('old', 160),\n",
              "  ('left', 122),\n",
              "  ('right', 114),\n",
              "  ('months', 105),\n",
              "  ('years', 103),\n",
              "  ('history', 103),\n",
              "  ('or', 97)],\n",
              " [('examination', 352),\n",
              "  ('left', 280),\n",
              "  ('blood', 259),\n",
              "  ('normal', 244),\n",
              "  ('right', 236),\n",
              "  ('dl', 224),\n",
              "  ('mg', 219),\n",
              "  ('ct', 205),\n",
              "  ('physical', 196),\n",
              "  ('mm', 160)],\n",
              " [('infection', 56),\n",
              "  ('disease', 55),\n",
              "  ('severe', 53),\n",
              "  ('right', 52),\n",
              "  ('left', 50),\n",
              "  ('tumor', 49),\n",
              "  ('syndrome', 45),\n",
              "  ('disorder', 43),\n",
              "  ('chronic', 40),\n",
              "  ('itis', 39)],\n",
              " [('follow', 146),\n",
              "  ('therapy', 81),\n",
              "  ('mg', 71),\n",
              "  ('function', 68),\n",
              "  ('care', 64),\n",
              "  ('surgical', 58),\n",
              "  ('up', 56),\n",
              "  ('monitoring', 53),\n",
              "  ('blood', 47),\n",
              "  ('pain', 42)])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S_top = sorted(S.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "O_top = sorted(O.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "A_top = sorted(A.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "P_top = sorted(P.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "\n",
        "S_top, O_top, A_top, P_top"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgcylc5LNDsH",
        "outputId": "1271c339-f39f-4b95-eb86-891b4fdaf8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('SIGN_SYMPTOM', 3132),\n",
              "  ('HISTORY', 2180),\n",
              "  ('DETAILED_DESCRIPTION', 1703),\n",
              "  ('DISEASE_DISORDER', 1549),\n",
              "  ('BIOLOGICAL_STRUCTURE', 1521),\n",
              "  ('MEDICATION', 922),\n",
              "  ('AGE', 837),\n",
              "  ('DURATION', 837),\n",
              "  ('DATE', 763),\n",
              "  (\"['AGE', '-']\", 702)],\n",
              " [('DIAGNOSTIC_PROCEDURE', 9296),\n",
              "  ('LAB_VALUE', 7863),\n",
              "  ('BIOLOGICAL_STRUCTURE', 5335),\n",
              "  ('SIGN_SYMPTOM', 4034),\n",
              "  ('DETAILED_DESCRIPTION', 3483),\n",
              "  ('DISEASE_DISORDER', 1758),\n",
              "  (\"['LAB_VALUE', '/']\", 1595),\n",
              "  (\"['LAB_VALUE', '.']\", 1007),\n",
              "  ('THERAPEUTIC_PROCEDURE', 838),\n",
              "  (\"['DIAGNOSTIC_PROCEDURE', '笆‘xamination']\", 696)],\n",
              " [('DISEASE_DISORDER', 4237),\n",
              "  ('BIOLOGICAL_STRUCTURE', 938),\n",
              "  ('DETAILED_DESCRIPTION', 861),\n",
              "  ('SIGN_SYMPTOM', 857),\n",
              "  ('DIAGNOSTIC_PROCEDURE', 481),\n",
              "  ('THERAPEUTIC_PROCEDURE', 264),\n",
              "  ('LAB_VALUE', 156),\n",
              "  (\"['DISEASE_DISORDER', '笆（nfection']\", 146),\n",
              "  (\"['DISEASE_DISORDER', '笆‥isease']\", 132),\n",
              "  ('MEDICATION', 129)],\n",
              " [('THERAPEUTIC_PROCEDURE', 1517),\n",
              "  ('MEDICATION', 1495),\n",
              "  ('DIAGNOSTIC_PROCEDURE', 1062),\n",
              "  ('DISEASE_DISORDER', 862),\n",
              "  ('DETAILED_DESCRIPTION', 584),\n",
              "  ('DOSAGE', 544),\n",
              "  ('SIGN_SYMPTOM', 478),\n",
              "  ('NONBIOLOGICAL_LOCATION', 457),\n",
              "  ('BIOLOGICAL_STRUCTURE', 440),\n",
              "  (\"['CLINICAL_EVENT', '笆’ollow']\", 272)])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Personalised Classification Algorithm"
      ],
      "metadata": {
        "id": "0oqUTFFVzxj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=ds[\"train\"][\"input\"]\n",
        "sentences=[]\n",
        "for i in train_df:\n",
        "  i=i.split(\"\\n\")\n",
        "  for a in i:\n",
        "    sentences.append(a)\n",
        "\n"
      ],
      "metadata": {
        "id": "6WfeTlDOQYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "formatted_data = []\n",
        "for item in sentences:\n",
        "\n",
        "  formatted_data.append({'text': item})\n"
      ],
      "metadata": {
        "id": "xVx-8WXzR6ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(formatted_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeNETEKHTt9v",
        "outputId": "05bb766b-ed97-46b7-adf4-facab7875ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqB_Y2TEQpaS",
        "outputId": "8d3766c0-f114-4442-fe6b-39156b173d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Doctor: Hello, how can I help you today?'},\n",
              " {'text': \"Patient: My son has been having some issues with speech and development. He's 13 years old now.\"},\n",
              " {'text': 'Doctor: I see. Can you tell me more about his symptoms? Does he have any issues with muscle tone or hypotonia?'},\n",
              " {'text': \"Patient: No, he doesn't have hypotonia. But he has mild to moderate speech and developmental delay, and he's been diagnosed with attention deficit disorder.\"},\n",
              " {'text': \"Doctor: Thank you for sharing that information. We'll run some tests, including an MRI, to get a better understanding of your son's condition. \"}]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Load the tokenizer and model for NER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"blaze999/Medical-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"blaze999/Medical-NER\")\n",
        "\n",
        "# Create an NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# Updated words per category as provided\n",
        "category_words = {\n",
        "    'S': ['pain', 'of', 'year', 'old', 'left', 'right', 'months', 'years', 'history', 'or'],\n",
        "    'O': ['examination', 'left', 'blood', 'normal', 'right', 'dl', 'mg', 'ct', 'physical', 'mm'],\n",
        "    'A': ['infection', 'disease', 'severe', 'right', 'left', 'tumor', 'syndrome', 'disorder', 'chronic', 'itis'],\n",
        "    'P': ['follow', 'therapy', 'mg', 'function', 'care', 'surgical', 'up', 'monitoring', 'blood', 'pain']\n",
        "}\n",
        "\n",
        "# Build word_to_category mapping\n",
        "word_to_category = {}\n",
        "for category, words in category_words.items():\n",
        "    for word in words:\n",
        "        if word in word_to_category:\n",
        "            word_to_category[word].append(category)\n",
        "        else:\n",
        "            word_to_category[word] = [category]\n",
        "\n",
        "# Updated entities per category as provided\n",
        "category_entities = {\n",
        "    'S': ['SIGN_SYMPTOM', 'HISTORY', 'DETAILED_DESCRIPTION', 'DISEASE_DISORDER', 'BIOLOGICAL_STRUCTURE',\n",
        "          'MEDICATION', 'AGE', 'DURATION', 'DATE', \"['AGE', '-']\"],\n",
        "    'O': ['DIAGNOSTIC_PROCEDURE', 'LAB_VALUE', 'BIOLOGICAL_STRUCTURE', 'SIGN_SYMPTOM', 'DETAILED_DESCRIPTION',\n",
        "          'DISEASE_DISORDER', 'THERAPEUTIC_PROCEDURE', 'MEDICATION', 'AREA', 'DOSAGE', \"['LAB_VALUE', '/']\",\n",
        "          \"['LAB_VALUE', '.']\", \"['DIAGNOSTIC_PROCEDURE', '笆‘xamination']\"],\n",
        "    'A': ['DISEASE_DISORDER', 'BIOLOGICAL_STRUCTURE', 'DETAILED_DESCRIPTION', 'SIGN_SYMPTOM',\n",
        "          'DIAGNOSTIC_PROCEDURE', 'THERAPEUTIC_PROCEDURE', 'LAB_VALUE', 'MEDICATION', 'SEVERITY', 'HISTORY',\n",
        "          \"['DISEASE_DISORDER', '笆（nfection']\", \"['DISEASE_DISORDER', '笆‥isease']\"],\n",
        "    'P': ['THERAPEUTIC_PROCEDURE', 'MEDICATION', 'DIAGNOSTIC_PROCEDURE', 'DISEASE_DISORDER', 'DETAILED_DESCRIPTION',\n",
        "          'DOSAGE', 'SIGN_SYMPTOM', 'NONBIOLOGICAL_LOCATION', 'BIOLOGICAL_STRUCTURE', 'CLINICAL_EVENT',\n",
        "          \"['CLINICAL_EVENT', '笆’ollow']\"]\n",
        "}\n",
        "\n",
        "# Build entity_to_category mapping\n",
        "entity_to_category = {}\n",
        "for category, entities in category_entities.items():\n",
        "    for entity in entities:\n",
        "        if entity in entity_to_category:\n",
        "            entity_to_category[entity].append(category)\n",
        "        else:\n",
        "            entity_to_category[entity] = [category]\n",
        "\n",
        "# Function to classify a sentence\n",
        "def classify_sentence(sentence):\n",
        "    doc = nlp(sentence.lower())\n",
        "    word_category_counts = {}\n",
        "\n",
        "    # Step 1: Word Check\n",
        "    for token in doc:\n",
        "        word = token.lemma_\n",
        "        if word in word_to_category:\n",
        "            categories = word_to_category[word]\n",
        "            for category in categories:\n",
        "                word_category_counts[category] = word_category_counts.get(category, 0) + 1\n",
        "\n",
        "    if word_category_counts:\n",
        "        # Find categories with the highest counts\n",
        "        max_count = max(word_category_counts.values())\n",
        "        top_categories = [cat for cat, count in word_category_counts.items() if count == max_count]\n",
        "        if len(top_categories) == 1:\n",
        "            return top_categories[0]\n",
        "        else:\n",
        "            # Tie in word categories, proceed to Entity Check using NER model\n",
        "            entities = extract_entities(sentence)\n",
        "    else:\n",
        "        # No word categories found, proceed to Entity Check using NER model\n",
        "        entities = extract_entities(sentence)\n",
        "\n",
        "    # Step 2: Entity Check\n",
        "    entity_category_counts = {}\n",
        "    for entity_label in entities:\n",
        "        if entity_label in entity_to_category:\n",
        "            categories = entity_to_category[entity_label]\n",
        "            for category in categories:\n",
        "                entity_category_counts[category] = entity_category_counts.get(category, 0) + 1\n",
        "\n",
        "    if entity_category_counts:\n",
        "        # Find categories with the highest counts\n",
        "        max_count = max(entity_category_counts.values())\n",
        "        top_entity_categories = [cat for cat, count in entity_category_counts.items() if count == max_count]\n",
        "        if len(top_entity_categories) == 1:\n",
        "            return top_entity_categories[0]\n",
        "        else:\n",
        "            # Tie in entity categories, classify as \"Not sure\"\n",
        "            return \"Not sure\"\n",
        "    else:\n",
        "        # No entity categories found, classify as \"Other\"\n",
        "        return \"Other\"\n",
        "\n",
        "# Function to extract entities using the NER model\n",
        "def extract_entities(sentence):\n",
        "    ner_results = ner_pipeline(sentence)\n",
        "    extracted_entities = set()\n",
        "    for entity in ner_results:\n",
        "        label = entity['entity_group']\n",
        "        extracted_entities.add(label)\n",
        "    return list(extracted_entities)\n",
        "\n",
        "row=[]\n",
        "for item in formatted_data[:10000]:\n",
        "    sentence = item[\"text\"]\n",
        "    category = classify_sentence(sentence)\n",
        "    if category==\"Not sure\":\n",
        "      continue\n",
        "    row.append([[sentence],[category]])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8Q57klWRScU",
        "outputId": "242104ce-f5f1-4883-8a64-2f698ac5d4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(row)\n",
        "S_df = pd.DataFrame(row, columns=[\"Entity\",\"Word\"])\n",
        "S_path = '/content/drive/MyDrive/MT_speech/SOAP_Files/sentences.csv'\n",
        "S_df.to_csv(S_path, index=False)"
      ],
      "metadata": {
        "id": "_rjdPMooU1Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bj08EnltU_87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
